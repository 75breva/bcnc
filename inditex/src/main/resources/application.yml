
server:
  port: 8181

spring:
  application:
    name: inditex

#  datasource:
#    url: jdbc:h2:mem:bcnc
#    username: sa
#    password:
#    driverClassName: org.h2.Driver
#    data: classpath:data.sql
#    schema: classpath:schema.sql
#    sql-script-encoding: UTF-8
#
#  jpa:
#    database: h2
#    database-platform: org.hibernate.dialect.H2Dialect
#    show-sql: true
#    hibernate:
#      ddl-auto: update  # create-drop (es bueno si en elos test no indicas create table if not exits), validate (no arranca si no lo valida)
#      use_sql_comments: true
#      format_sql: true
#
#  sql:
#    init:
#      mode: always
#      data-locations: classpath:data.sql
#  h2:
#    console:
#      enabled: true
#      path: /h2-console
#      settings.trace: false
#      settings.web-allow-others: false
#
#
#logging:
#  level:
#    org.hibernate.SQL: debug
#    org.hibernate.type.descriptor.sql.BasicBinder: trace
  jpa:
    database: postgresql
    database-platform: org.hibernate.dialect.PostgreSQLDialect
    properties:
      hibernate:
        default_schema: bcnc  # Con esto simplemente no funcionaba bien., define el esquema que se utiliza por defecto cuando no se especifica expl√≠citamente un esquema en las entidades
    show-sql: true
    hibernate.ddl-auto: update
  datasource:
    url: jdbc:postgresql://localhost:5432/bcnc?currentSchema=bcnc  #  IMPORTANTISIMO para que no intente buscar datos en bbdd:bcnc pero con schema:public, si no que lo haga en el schema:bcnc
    username: admin
    password: admin
    driver-class-name: org.postgresql.Driver

  kafka:
    bootstrap-servers: localhost:29092   # utilizando DOCKER no me funciona el puerto 9092
    consumer:
      #      bootstrap-servers: localhost:9092
      group-id: group_id
      auto-offset-reset: earliest
      #      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      #      value-deserializer: org.springframework.kafka.support.serializer.JsonDeserializer
      keyDeserializer: org.apache.kafka.common.serialization.StringDeserializer # keyDeserializer o key-deserializer  es lo mismo. Interesante
      valueDeserializer: io.confluent.kafka.serializers.KafkaAvroDeserializer # Ahora indico consumir objeto avro

      properties:
        #        spring.deserializer.value.delegate.class: org.apache.kafka.common.serialization.JsonDeserializer # lo comento pk paso a estar con avro en vez de un json
        spring.deserializer.value.delegate.class: io.confluent.kafka.serializers.KafkaAvroDeserializer  # Ahora indico consumir desde avro
        #        spring.json.value.default.type: com.bncn.inditex.events.Event  # lo comento pk paso a estar con avro
        #        spring.json.trusted.packages: "*"                              # lo comento pk paso a estar con avro
        schema:
          registry:
            url: "http://127.0.0.1:8081"  # URL del Schema Registry
        specific:
          avro:
            reader: "true"
    producer:
      #      bootstrap-servers: localhost:9092
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: io.confluent.kafka.serializers.KafkaAvroSerializer  #AHORA estoy con avro y no se serializa JSON
      #      value-serializer: org.springframework.kafka.support.serializer.JsonSerializer  # lo comento pk paso a estar con avro
      properties:
        #        spring.kafka.producer.properties.spring.json.type.mapping: "event:com.bncn.inditex.events.Event"  # lo comento pk paso a estar con avro
        schema:
          registry:
            url: "http://127.0.0.1:8081" # URL del Schema Registry
logging:
  level:
    org.hibernate.SQL: debug
    org.hibernate.type.descriptor.sql.BasicBinder: trace